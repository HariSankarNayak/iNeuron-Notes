{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c42cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import string\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06067155",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='word_task2.log',encoding = \"utf8\", level = logging.INFO, format = \"%(asctime)s \\t %(levelname)s \\t %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e6f2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class words:\n",
    "    \n",
    "    def __init__(self, fileName, db_Name, table_Name):\n",
    "        logging.info(f\"Object Created for dataset {fileName}!!!\")\n",
    "        \n",
    "        self.fileName = fileName\n",
    "        self.db_Name = db_Name\n",
    "        self.table_Name = table_Name\n",
    "        \n",
    "        self.db = sqlite3.connect(db_Name+\".db\")\n",
    "        self.cursor = self.db.cursor()\n",
    "        \n",
    "        logging.info(f\"Database Created for dataset {fileName}!!!\")\n",
    "     \n",
    "    def __repr__(self):\n",
    "        return (f\"This is the object of file {self.fileName}\")\n",
    "        \n",
    "    def data_insertion(self):\n",
    "        \"\"\"Reads the given data and store it into list and returns it. \"\"\"\n",
    "    \n",
    "        try:\n",
    "            querry = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "            self.cursor.execute(querry)\n",
    "            list_tablenames = [i[0] for i in self.cursor.fetchall()]\n",
    "            \n",
    "            if self.table_Name in list_tablenames:\n",
    "                self.cursor.execute(f\"DROP table {self.table_Name}\")\n",
    "                self.cursor.execute(f\"create table {self.table_Name}(col1 text)\")\n",
    "                logging.info(f\"Table {self.table_Name} created successfully!!\")                   \n",
    "            else:\n",
    "                self.cursor.execute(f\"create table {self.table_Name}(col1 text)\")\n",
    "                logging.info(f\"Table {self.table_Name} created successfully!!\")\n",
    "                \n",
    "            logging.info(f\"Reading Dataset: {self.fileName}\")\n",
    "            with open(self.fileName, \"r+\", encoding = \"utf8\") as f:\n",
    "                data = csv.reader(f, delimiter = \"\\n\")\n",
    "                for i in data:\n",
    "                    if i[0][-1]==\"_\":\n",
    "                        i[0]=i[0].strip(\"_\")\n",
    "                    self.cursor.execute(\"INSERT into \"+self.table_Name+\" VALUES(?);\",i)\n",
    "                \n",
    "                self.cursor.execute(f\"SELECT * from {self.table_Name}\")\n",
    "                data = self.cursor.fetchall()\n",
    "                self.db.commit()\n",
    "            logging.info(f\"Reading Dataset {self.fileName} is successful....!!\")\n",
    "            return data\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.warning(\"Problem Occured while reading the data...!!\")\n",
    "            logging.exception(\"Error:\", e)\n",
    "            \n",
    "    def occurance_counter(self):\n",
    "        \"\"\"It counts the occurrances of each word. Returns the list of tuples of word and its count. \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Counting Occurance of each word in the dataset {self.fileName}\")\n",
    "            self.cursor.execute(f\"SELECT col1, count(col1) from {self.table_Name} GROUP BY col1\")\n",
    "            final_data = self.cursor.fetchall()\n",
    "            return final_data\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Problem occurred in occurance_counter() method\")\n",
    "            logging.exception(e)\n",
    "    \n",
    "    def starting_occurance_counter(self):\n",
    "        \"\"\"It counts the words starting with same alphabet and returns list of tuples of alphabet and its count.\"\"\"\n",
    "        logging.info(f\"Counting no. of words starting with same alphabet for dataset {self.fileName}\")\n",
    "        try:\n",
    "            counter = []\n",
    "            for i in string.ascii_lowercase:\n",
    "                self.cursor.execute(f\"SELECT count(col1) from {self.table_Name} WHERE col1 LIKE '{i}%'\")\n",
    "                count = self.cursor.fetchall()\n",
    "                counter.append((i,count[0][0]))\n",
    "            return counter\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Problem occurred in starting_occurance_counter() method\")\n",
    "            logging.exception(e) \n",
    "    \n",
    "    def word_extracter(self,extract_table):\n",
    "        \"\"\"This fuction removes all the digits and punctuation returns only words\"\"\"\n",
    "        logging.info(f\"Extracting words from the dataset {self.fileName} after removing digits and punctuations\")\n",
    "        \n",
    "        unwanted  = string.digits + string.punctuation\n",
    "        \n",
    "        querry = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "        self.cursor.execute(querry)\n",
    "        list_tablenames = [i[0] for i in self.cursor.fetchall()]\n",
    "            \n",
    "        if extract_table in list_tablenames:\n",
    "            self.cursor.execute(f\"DROP table {extract_table}\")\n",
    "            self.cursor.execute(f\"create table {extract_table}(col1 text)\")                   \n",
    "        else:\n",
    "            self.cursor.execute(f\"create table {extract_table}(col1 text)\")\n",
    "        \n",
    "        with open(self.fileName, \"r+\",encoding = \"utf8\") as f:\n",
    "            new_data = csv.reader(f,delimiter = \"\\n\")\n",
    "            for i in new_data:\n",
    "                for j in unwanted:\n",
    "                    i[0] = i[0].replace(j,\"\")\n",
    "                if i[0]==\"\":\n",
    "                    continue\n",
    "                else:\n",
    "                    self.cursor.execute(\"INSERT into \"+extract_table+\" VALUES(?);\",i)\n",
    "        self.cursor.execute(f\"SELECT * from {extract_table}\")\n",
    "        data2 = self.cursor.fetchall() \n",
    "        self.db.commit()\n",
    "        return data2\n",
    "    \n",
    "    def dataset_zipper(self, *args):\n",
    "        \"\"\"It returns the list of tuples of all dataset where each records of dataset is mapped to other dataset \\n\n",
    "        to form one single tuple\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Zipping all {len(args)} datasets into one........\")\n",
    "            zipped = list(zip(*args))\n",
    "            logging.info(\"Datasets zipped into one dataset successful\")\n",
    "            return zipped\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Problem occurred while zipping datasets\")\n",
    "            logging.exception(e)\n",
    "            \n",
    "    \n",
    "    def sqlite_database(self,data):\n",
    "        \"\"\"This function helps to create in-memory database\"\"\"\n",
    "        logging.info(\"Creating in-memory database for consolidated datasets\")\n",
    "        \n",
    "        try: \n",
    "\n",
    "            table_name= input(\"Enter table name: \")\n",
    "            \n",
    "            querry = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "            self.cursor.execute(querry)\n",
    "            list_tablenames = [i[0] for i in self.cursor.fetchall()]\n",
    "\n",
    "            if table_name in list_tablenames:\n",
    "                self.cursor.execute(f\"DROP table {table_name}\")\n",
    "                self.cursor.execute(f\"create table {table_name}(col1 text, col2 text, col3 text, col4 text, col5 text)\")\n",
    "                logging.info(f\"Table {table_name} created successfully!!!\")                   \n",
    "            else:\n",
    "                self.cursor.execute(f\"create table {table_name}(col1 text, col2 text, col3 text, col4 text, col5 text)\")\n",
    "                logging.info(f\"Table {table_name} created successfully!!!\")\n",
    "\n",
    "            for i in data:\n",
    "                self.cursor.execute(\"insert into \"+table_name+\" values(?,?,?,?,?)\",i)\n",
    "            self.cursor.execute(f\"select * from {table_name}\")\n",
    "            data = self.cursor.fetchall()\n",
    "            self.db.commit()\n",
    "            logging.info(\"Data inserted into database successfully..!!!!!\")\n",
    "            return data\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(\"Problem occured while database creation\")\n",
    "            logging.exception(\"Error: \",e)\n",
    "        \n",
    "    def db_close(self):\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c8a3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object for file 'vocab.enron.txt'\n",
    "obj1 = words(\"E:\\\\Shivansh\\\\iNeuron\\\\New folder\\\\Data\\\\vocab.enron.txt\",\"bag_of_words\",\"enron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "814e1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object for file 'vocab.kos.txt'\n",
    "obj2 = words(\"E:\\\\Shivansh\\\\iNeuron\\\\New folder\\\\Data\\\\vocab.kos.txt\",\"bag_of_words\",\"kos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8873683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object for file 'vocab.nips.txt'\n",
    "obj3 = words(\"E:\\\\Shivansh\\\\iNeuron\\\\New folder\\\\Data\\\\vocab.nips.txt\",\"bag_of_words\",\"nips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc884859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object for file 'vocab.nytimes.txt'\n",
    "obj4 = words(\"E:\\\\Shivansh\\\\iNeuron\\\\New folder\\\\Data\\\\vocab.nytimes.txt\",\"bag_of_words\",\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bc7f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object for file 'vocab.pubmed.txt'\n",
    "obj5 = words(\"E:\\\\Shivansh\\\\iNeuron\\\\New folder\\\\Data\\\\vocab.pubmed.txt\",\"bag_of_words\",\"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719b2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 'vocab.enron.txt'\n",
    "enron = obj1.data_insertion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc735d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 'vocab.kos.txt'\n",
    "kos = obj2.data_insertion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d229e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 'vocab.nips.txt'\n",
    "nips = obj3.data_insertion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3a3edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 'vocab.nytimes.txt'\n",
    "nytimes = obj4.data_insertion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684a10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 'vocab.pubmed.txt'\n",
    "pubmed = obj5.data_insertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1091e6",
   "metadata": {},
   "source": [
    "##### Finding occurrrence of each word in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3107bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_enron = obj1.occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in occur_enron:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e99c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_kos = obj2.occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8aa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in occur_kos:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7fcffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_nips = obj3.occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85965dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in occur_nips:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8fe9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_nytimes = obj4.occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in occur_nytimes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceb0b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_pubmed = obj5.occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in occur_pubmed:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8be9d",
   "metadata": {},
   "source": [
    "##### Finding count of each words starting with same alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddd6a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_enron = obj1.starting_occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in start_enron:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "952e51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_kos = obj2.starting_occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ade50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in start_kos:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d73bd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nips = obj3.starting_occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in start_nips:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c04bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_nytimes = obj4.starting_occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8962bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in start_nytimes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "847a5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pubmed = obj5.starting_occurance_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in start_pubmed:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf7435",
   "metadata": {},
   "source": [
    "##### Extracting only words after removing all the puctuations and digits from 'vocab.pubmed.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7499c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pubmed = obj5.word_extracter(\"extract_pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae63085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in extract_pubmed:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc977f0c",
   "metadata": {},
   "source": [
    "##### Zipping all five datasets into one and creating a in-memory database for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a434c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [i[0] for i in enron]\n",
    "l2 = [i[0] for i in kos]\n",
    "l3 = [i[0] for i in nips]\n",
    "l4 = [i[0] for i in nytimes]\n",
    "l5 = [i[0] for i in pubmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dfc8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = obj1.dataset_zipper(l1,l2,l3,l4,l5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75498fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aaa', 'aarp', 'a2i', 'aah', '>=') \r\n",
      "('aaas', 'abandon', 'aaa', 'aahed', '>>') \r\n",
      "('aactive', 'abandoned', 'aaai', 'aaron', '>>>') \r\n",
      "('aadvantage', 'abandoning', 'aapo', 'aback', '>/=') \r\n",
      "('aaker', 'abb', 'aat', 'abacus', '->') \r\n",
      "('aap', 'abc', 'aazhang', 'abajo', '--') \r\n",
      "('aapg', 'abcs', 'abandonment', 'abalone', '-->') \r\n",
      "('aaron', 'abdullah', 'abbott', 'abandon', '-/-') \r\n",
      "('aarp', 'ability', 'abbreviated', 'abandoned', '-/+') \r\n",
      "('aas', 'aboard', 'abcde', 'abandoning', '/-') \r\n",
      "('aau', 'abortion', 'abe', 'abandonment', '/+-') \r\n",
      "('ab1890', 'abortions', 'abeles', 'abandono', '..') \r\n",
      "('ab1x', 'abraham', 'abi', 'abarnard', '...') \r\n",
      "('ab31x', 'abrams', 'abilistic', 'abashed', '+-') \r\n",
      "('aba', 'abroad', 'abilities', 'abate', '+/') \r\n",
      "('abacus', 'absence', 'ability', 'abated', '+/--') \r\n",
      "('abag', 'absent', 'abl', 'abatement', '+/?') \r\n",
      "('abalone', 'absentee', 'able', 'abating', '+/+') \r\n",
      "('abandon', 'absolute', 'ables', 'abbey', '++') \r\n",
      "('abandoned', 'absolutely', 'ablex', 'abbot', '+++') \r\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    print(zipped[i],\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e786187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter table name: zipped_dataset\n"
     ]
    }
   ],
   "source": [
    "db_data= obj1.sqlite_database(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in db_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09d56ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1e2fa735110>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1.cursor.execute(\"select name from sqlite_master where type='table'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a87e2144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enron',),\n",
       " ('kos',),\n",
       " ('nips',),\n",
       " ('nytimes',),\n",
       " ('pubmed',),\n",
       " ('extract_pubmed',),\n",
       " ('zipped_dataset',)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj1.cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "facf3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj5.db_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd2e3ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the object of file E:\\Shivansh\\iNeuron\\New folder\\Data\\vocab.enron.txt\n"
     ]
    }
   ],
   "source": [
    "print(obj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d86d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
